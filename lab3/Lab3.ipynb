{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0nAlt_2u9Fb"
   },
   "source": [
    "Lab 3\n",
    "==========\n",
    "\n",
    "This lab will walk you through the common tasks needed to train a neural network using PyTorch. It has been adapted from the PyTorch quickstart guide. For further information on any of these tools, you can find the whole tutorial here: https://pytorch.org/tutorials/beginner/basics/intro.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2vzkrIaHu9Fc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SauRt7qHyWtA"
   },
   "source": [
    "Tensors\n",
    "----------\n",
    "\n",
    "Tensors are a specialized data structure that are very similar to arrays\n",
    "and matrices. In PyTorch, we use tensors to encode the inputs and\n",
    "outputs of a model, as well as the model's parameters.\n",
    "\n",
    "Tensors are similar to NumPy's ndarrays, except\n",
    "that tensors can run on GPUs or other hardware accelerators. In fact,\n",
    "tensors and NumPy arrays can often share the same underlying memory,\n",
    "eliminating the need to copy data. Tensors are also\n",
    "optimized for the automatic differentiation needed to train neural networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34RjozJ9xLmi"
   },
   "source": [
    "Initializing a Tensor\n",
    "=====================\n",
    "\n",
    "Tensors can be initialized in various ways. Take a look at the following\n",
    "examples:\n",
    "\n",
    "**Directly from data**\n",
    "\n",
    "Tensors can be created directly from data. The data type is\n",
    "automatically inferred.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-kRHCS3kxLmi"
   },
   "outputs": [],
   "source": [
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkuioW0QxLmi"
   },
   "source": [
    "**From a NumPy array**\n",
    "\n",
    "Tensors can be created from NumPy arrays (and vice versa).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "MlNY8kHDxLmj",
    "outputId": "e52723ae-2efd-4822-b2e3-2ea97315580e"
   },
   "outputs": [],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npXLXtUtxLmj"
   },
   "source": [
    "**With random or constant values:**\n",
    "\n",
    "`shape` is a tuple of tensor dimensions. In the functions below, it\n",
    "determines the dimensionality of the output tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "ZJNIxwpwxLmj",
    "outputId": "c90503c9-3da4-4acc-a880-6672bdceb082"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.1900, 0.9561, 0.8868],\n",
      "        [0.0874, 0.8118, 0.2660]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")\n",
    "\n",
    "#TODO: Create a one-dimensional PyTorch tensor that holds 100 ones\n",
    "hundred_ones = torch.ones((100))\n",
    "print(hundred_ones)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VqzzwT5oyZ-o"
   },
   "source": [
    "\n",
    "Working with data\n",
    "-----------------\n",
    "\n",
    "PyTorch has two [primitives to work with\n",
    "data](https://pytorch.org/docs/stable/data.html):\n",
    "`torch.utils.data.DataLoader` and `torch.utils.data.Dataset`. `Dataset`\n",
    "stores the samples and their corresponding labels, and `DataLoader`\n",
    "wraps an iterable around the `Dataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nORnnr67u9Fc"
   },
   "source": [
    "PyTorch offers domain-specific libraries such as\n",
    "[TorchText](https://pytorch.org/text/stable/index.html),\n",
    "[TorchVision](https://pytorch.org/vision/stable/index.html), and\n",
    "[TorchAudio](https://pytorch.org/audio/stable/index.html), all of which\n",
    "include datasets. For this tutorial, we will be using a TorchVision\n",
    "dataset.\n",
    "\n",
    "The `torchvision.datasets` module contains `Dataset` objects for many\n",
    "real-world vision data like CIFAR, COCO ([full list\n",
    "here](https://pytorch.org/vision/stable/datasets.html)). In this\n",
    "tutorial, we use the FashionMNIST dataset. Every TorchVision `Dataset`\n",
    "includes two arguments: `transform` and `target_transform` to modify the\n",
    "samples and labels respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "zmFGoQLMu9Fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26.4M/26.4M [00:05<00:00, 4.52MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29.5k/29.5k [00:00<00:00, 150kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.42M/4.42M [00:01<00:00, 4.33MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.15k/5.15k [00:00<00:00, 636kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tW6MXbF6u9Fd"
   },
   "source": [
    "We pass the `Dataset` as an argument to `DataLoader`. This wraps an\n",
    "iterable over our dataset, and supports automatic batching, sampling,\n",
    "shuffling and multiprocess data loading. Here we define a batch size. Each element in the dataloader iterable will return a batch of that many features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "UI3iPdQnu9Fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([100, 1, 28, 28])\n",
      "Shape of y: torch.Size([100]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "#TO DO: set a reasonable batch size\n",
    "batch_size = 100\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "#TO DO: Create the test data loader\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zA-Qhc8Yu9Fd"
   },
   "source": [
    "------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNk_DgKPu9Fd"
   },
   "source": [
    "Creating Models\n",
    "===============\n",
    "\n",
    "To define a neural network in PyTorch, we create a class that inherits\n",
    "from\n",
    "[nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html).\n",
    "To accelerate operations in the neural network, we move it to\n",
    "the GPU or MPS if available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "egpPVCsYu9Fd",
    "outputId": "433c32cc-e51e-4085-98fb-5b349fcf095f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6S3fODhzuhm"
   },
   "source": [
    "Now, we can define our class.We define the layers of the network in the `__init__` function and specify how data will pass through the network in the `forward`\n",
    "function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JUhWHE95zuQC",
    "outputId": "a2275e4b-28c6-4e40-f9ac-dc998f00aba0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SampleNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "class SampleNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "sample_model = SampleNetwork().to(device)\n",
    "print(sample_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKxJndc-PnPJ"
   },
   "source": [
    "Now it's your turn! Following the example above, create a network with three hidden layers. Each hidden layer should be followed by a ReLU layer (although the output should not). You may wish to make the layer size smaller than 512 or to have different hidden layers of different sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "h3s3gCiWPmZy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qq_dwC20u9Fe"
   },
   "source": [
    "Read more about [building neural networks in\n",
    "PyTorch](buildmodel_tutorial.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "InjX9Acbu9Fe"
   },
   "source": [
    "------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eV4jEuJPu9Fe"
   },
   "source": [
    "Optimizing the Model Parameters\n",
    "===============================\n",
    "\n",
    "To train a model, we need a [loss\n",
    "function](https://pytorch.org/docs/stable/nn.html#loss-functions) and an\n",
    "[optimizer](https://pytorch.org/docs/stable/optim.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "pC2C8nXlu9Fe"
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6IFOvl5u9Fe"
   },
   "source": [
    "In a single training loop, the model makes predictions on the training\n",
    "dataset (fed to it in batches), and backpropagates the prediction error\n",
    "to adjust the model\\'s parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "PdTwib2su9Fe"
   },
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KevVnlSNu9Fe"
   },
   "source": [
    "We also check the model\\'s performance against the test dataset to\n",
    "ensure it is learning. The following code runs a test set (one batch at a time) through the network and calculates lost. Add extra code in this function that computes and prints the accuracy of our model over the whole test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "aO0_GHWVu9Fe"
   },
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    #TODO: Compute and print accuracy\n",
    "    correct = correct / size\n",
    "    print(correct)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1-j08O2u9Fe"
   },
   "source": [
    "The training process is conducted over several iterations (*epochs*).\n",
    "During each epoch, the model learns parameters to make better\n",
    "predictions. We can print the model\\'s accuracy and loss at each epoch;\n",
    "we\\'d like to see the accuracy increase and the loss decrease with every\n",
    "epoch.\n",
    "\n",
    "Fill out the following loop to run our training function on the training set and our testing function on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "NkS8sz1Pu9Fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.304855  [  100/60000]\n",
      "loss: 2.307618  [10100/60000]\n",
      "loss: 2.290003  [20100/60000]\n",
      "loss: 2.304931  [30100/60000]\n",
      "loss: 2.301395  [40100/60000]\n",
      "loss: 2.298408  [50100/60000]\n",
      "0.0712\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.304855  [  100/60000]\n",
      "loss: 2.307618  [10100/60000]\n",
      "loss: 2.290003  [20100/60000]\n",
      "loss: 2.304931  [30100/60000]\n",
      "loss: 2.301395  [40100/60000]\n",
      "loss: 2.298408  [50100/60000]\n",
      "0.0712\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.304855  [  100/60000]\n",
      "loss: 2.307618  [10100/60000]\n",
      "loss: 2.290003  [20100/60000]\n",
      "loss: 2.304931  [30100/60000]\n",
      "loss: 2.301395  [40100/60000]\n",
      "loss: 2.298408  [50100/60000]\n",
      "0.0712\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.304855  [  100/60000]\n",
      "loss: 2.307618  [10100/60000]\n",
      "loss: 2.290003  [20100/60000]\n",
      "loss: 2.304931  [30100/60000]\n",
      "loss: 2.301395  [40100/60000]\n",
      "loss: 2.298408  [50100/60000]\n",
      "0.0712\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.304855  [  100/60000]\n",
      "loss: 2.307618  [10100/60000]\n",
      "loss: 2.290003  [20100/60000]\n",
      "loss: 2.304931  [30100/60000]\n",
      "loss: 2.301395  [40100/60000]\n",
      "loss: 2.298408  [50100/60000]\n",
      "0.0712\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    #TO DO: Fill this in!\n",
    "    train(dataloader=train_dataloader, model=model, loss_fn=loss_fn, optimizer=optimizer)\n",
    "    test(dataloader=test_dataloader, model=model, loss_fn=loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQRET1YBu9Fe"
   },
   "source": [
    "Read more about [Training your model](optimization_tutorial.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wn6kH6beu9Fe"
   },
   "source": [
    "------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfEkeOUgu9Fe"
   },
   "source": [
    "Saving Models\n",
    "=============\n",
    "\n",
    "A common way to save a model is to serialize the internal state\n",
    "dictionary (containing the model parameters).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "gSpsMw7au9Fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3j3hLf4u9Fe"
   },
   "source": [
    "Loading Models\n",
    "==============\n",
    "\n",
    "The process for loading a model includes re-creating the model structure\n",
    "and loading the state dictionary into it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "SR1TS2CDu9Ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OoES9lzu9Ff"
   },
   "source": [
    "This model can now be used to make predictions. Add some code to print out the predicted and actual class for this test point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "PESeYVIZu9Ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:  Pullover\n",
      "Actual:  Ankle boot\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    #TO DO: print out the predicted and actual class\n",
    "    print(\"Predictions: \", classes[pred.argmax()])\n",
    "    print(\"Actual: \", classes[y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0R7bRGqu9Ff"
   },
   "source": [
    "Read more about [Saving & Loading your\n",
    "model](saveloadrun_tutorial.html).\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
